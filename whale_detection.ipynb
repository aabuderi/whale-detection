{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33bcb5fd-b120-48d7-8819-6b31a66cba41",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Right whale detection from hydrophone data\n",
    "This project is an implementation and analysis of a Convolutional Neural Network to accomplish the Kaggle challenge,\n",
    "\n",
    "## [The Marinexplore and Cornell University Whale Detection Challenge](https://www.kaggle.com/c/whale-detection-challenge)\n",
    "The goal of the project is to \"Create an algorithm to detect North Atlantic right whale calls from audio recordings, prevent collisions with shipping traffic\" (from Kaggle).\n",
    "\n",
    "### The data\n",
    "- 2-second .aiff sound clip\n",
    "- sample rate of 2 kHz\n",
    "- 20,000 training samples\n",
    "- 54,503 testing samples\n",
    "- Training labels given in separate csv\n",
    "\n",
    "### The implementation\n",
    "Since the data are given as uniform sound clips and the training data are labeled, we can take a supervised learning approach. By converting the audio data to spectrograms we can treat this similarly to an image classification problem. We use a Convolutional Neural Network using Pytorch for the data loader and model classes, and librosa for opening and manipulating the sound files. Pandas and Numpy also provide support for interpreting the training metadata csv.\n",
    "\n",
    "To "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4edb97-246f-4233-ac90-c5a510788251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import io\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib2 import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchaudio\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21bdc2-1b13-4ed5-a654-02e650c5c33b",
   "metadata": {},
   "source": [
    "The data was downloaded to my system and placed in the downloads section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e836e3f1-98d2-4977-a922-ae1cf713a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_whale_data = \"/Users/adambuderi/Downloads/whale-detection-challenge\"\n",
    "path_to_whale_data_csv = path_to_whale_data + \"/data/train.csv\"\n",
    "path_to_whale_data_audio = path_to_whale_data + \"/data/train/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1a7873-3665-4f76-94a7-f717ac521c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.getcwd())\n",
    "import whale_sound_dataset\n",
    "import audio_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ae07fb-76d4-4f67-a48f-df20a142a15d",
   "metadata": {},
   "source": [
    "I used this dictionary to set the parameters for the audio sample rate, duration, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4fbf9ed-5724-43b5-853d-b72f11bbafe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'number_of_epochs': 6, 'batch_size': 8, 'audio_sample_rate': 2000, 'audio_duration': 2000}\n"
     ]
    }
   ],
   "source": [
    "configuration_dict = {'number_of_epochs': 6, 'batch_size': 8, \n",
    "                      'audio_sample_rate': 2000, 'audio_duration': 2000}\n",
    "print(configuration_dict)  # printing actual configuration (after override in remote mode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbfc1fe-4082-4ea2-906b-bbcc7a576b21",
   "metadata": {},
   "source": [
    "Implement the Pytorch primitive [Dataset class](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for loading the audio files, converting them to mel spectrograms, and also loading their labels.\n",
    "\n",
    "<b>note</b>: This implementation is included in the notebook for clarity, but the version used by the notebook here is implemented in the file audio_classifier.py\n",
    "This is needed because of the [constraints of multithreading in Jupyter notebooks](https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror) I encountered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6276455-30fa-4da8-ad29-f8d93bf406bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class WhaleSoundDataSet(Dataset):\n",
    "    def __init__(self, csv_path, data_path, configuration_dict):\n",
    "        self.meta_df = None\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = configuration_dict.get('audio_duration', 2000)\n",
    "        self.sr = configuration_dict.get('audio_sample_rate', 2000)\n",
    "        self.channel = 2\n",
    "\n",
    "        self.meta_df = pd.read_csv(csv_path)\n",
    "        \n",
    "    # ----------------------------\n",
    "    # Number of items in dataset\n",
    "    # ----------------------------\n",
    "    def __len__(self):\n",
    "        return len(self.meta_df)\n",
    "\n",
    "    # ----------------------------\n",
    "\n",
    "    # Get i'th item in dataset\n",
    "    # ----------------------------\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the filename.\n",
    "        audio_file_path = self.data_path + self.meta_df.loc[idx, 'clip_name']\n",
    "        soundData, sr = librosa.load(audio_file_path, sr=2000, duration=2)\n",
    "        # Get the Class ID, either 0 (no whale) or 1 (whale)\n",
    "        # Get the Class ID.\n",
    "        class_id = self.meta_df.loc[idx, 'label']\n",
    "        \n",
    "        # This will convert audio files with two channels into one\n",
    "        soundData_mono = librosa.to_mono(soundData)\n",
    "        \n",
    "        # Convert audio to log-scale Mel spectrogram\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=soundData_mono, sr=sr)\n",
    "        \n",
    "        return mel_spectrogram, class_id\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1ba65-3d86-4fb7-b172-86c01ce8a51f",
   "metadata": {},
   "source": [
    "Instantiate the WhaleSoundDataSet class, then split that dataset between a training set and a testing set at an 80:20 ratio.\n",
    "\n",
    "Then, instantiate a DataLoader iterable around the datasets with a batch size of 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6750223f-e6ac-474b-af2b-18cd83299a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 24000\n",
      "Test set size: 6000\n"
     ]
    }
   ],
   "source": [
    "# dataset = WhaleSoundDataSet()\n",
    "train_set = whale_sound_dataset.WhaleSoundDataSet(path_to_whale_data_csv, path_to_whale_data_audio, configuration_dict)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(train_set)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, test_ds = random_split(train_set, [num_train, num_val])\n",
    "print(\"Train set size: \" + str(len(train_ds)))\n",
    "print(\"Test set size: \" + str(len(test_ds)))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size = configuration_dict.get('batch_size', 8), \n",
    "                                           shuffle = True, pin_memory=False, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size = configuration_dict.get('batch_size', 8), \n",
    "                                          shuffle = False, pin_memory=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b392cdf-6769-40bc-866c-34890ba22bc6",
   "metadata": {},
   "source": [
    "Define a custom Convolutional Neural Network\n",
    "\n",
    "The input is of the shape [num_channels, batch_size, img_height, img_width]\n",
    "\n",
    "<b>note</b>: This implementation is included in the notebook for clarity, but the version used by the notebook here is implemented in the file audio_classifier.py\n",
    "This is needed because of the [constraints of multithreading in Jupyter notebooks](https://stackoverflow.com/questions/41385708/multiprocessing-example-giving-attributeerror) I encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2b696b2-82c0-49b5-a2df-a0344e44edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        nn.init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        nn.init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        nn.init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Third Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        nn.init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e2b30-2fae-4dd4-952f-b80cf263941c",
   "metadata": {},
   "source": [
    "Instantiate the model, setting it up to use the GPU if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4f50987-c409-4e8d-a3df-30e1afb96730",
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = audio_classifier.AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cde721-f681-4204-92ed-af8938f9b8ac",
   "metadata": {},
   "source": [
    "The training loop for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aecde9bb-ab82-49d5-af57-c3abdf7035fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        # Repeat for each batch in the training set (sounds, sample_rate, inputs, labels)\n",
    "        for i, data in enumerate(train_dl):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[2].to(device), data[3].to(device)\n",
    "\n",
    "            #trying to reshape to make it 4d a la https://stackoverflow.com/questions/72808402/pytorch-identifying-batch-size-as-number-of-channels-in-conv2d-layer\n",
    "            inputs = torch.reshape(inputs, (8, 1, 128, 8))\n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "            if i % 1000 == 0:    # print every 1000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 1000))\n",
    "    \n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        acc = correct_prediction/total_prediction\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "    print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d7c0d5-a7ba-4280-bad2-e453d2f6b352",
   "metadata": {},
   "source": [
    "Testing loop for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c452389-0333-4c73-aa03-f51935e7401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def testing (model, val_dl, y_pred, y_true):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "        for data in val_dl:\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[2].to(device), data[3].to(device)\n",
    "            inputs = torch.reshape(inputs, (8, 1, 128, 8))\n",
    "        \n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # store the results for calculating auroc\n",
    "            y_true.append(labels.numpy()) #true labels\n",
    "            \n",
    "            \n",
    "            # Append the predicted probabilities to the list\n",
    "            y_pred.append(nn.functional.softmax(outputs, dim=1).numpy()[:, 1])\n",
    "    \n",
    "            # y_pred.append(torch.sigmoid(outputs).numpy()) # predicted labels\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "    \n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1bc98-247d-418b-9517-734c56e33b00",
   "metadata": {},
   "source": [
    "Running the training and testing loops. Note the creation of two lists to hold the predictions made by the model as well as the ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28c79f9b-6dca-485c-92f2-f6caf568842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.002\n",
      "[1,  1001] loss: 1.735\n",
      "[1,  2001] loss: 2.296\n",
      "Epoch: 0, Loss: 0.92, Accuracy: 0.70\n",
      "[2,     1] loss: 0.000\n",
      "[2,  1001] loss: 0.434\n",
      "[2,  2001] loss: 0.850\n",
      "Epoch: 1, Loss: 0.42, Accuracy: 0.78\n",
      "[3,     1] loss: 0.001\n",
      "[3,  1001] loss: 0.412\n",
      "[3,  2001] loss: 0.808\n",
      "Epoch: 2, Loss: 0.40, Accuracy: 0.80\n",
      "[4,     1] loss: 0.000\n",
      "[4,  1001] loss: 0.387\n",
      "[4,  2001] loss: 0.758\n",
      "Epoch: 3, Loss: 0.38, Accuracy: 0.81\n",
      "[5,     1] loss: 0.000\n",
      "[5,  1001] loss: 0.376\n",
      "[5,  2001] loss: 0.733\n",
      "Epoch: 4, Loss: 0.37, Accuracy: 0.82\n",
      "[6,     1] loss: 0.000\n",
      "[6,  1001] loss: 0.358\n",
      "[6,  2001] loss: 0.710\n",
      "Epoch: 5, Loss: 0.35, Accuracy: 0.83\n",
      "Finished Training\n",
      "Accuracy: 0.83, Total items: 6000\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "training(myModel, train_loader, configuration_dict.get('number_of_epochs', 6))\n",
    "# Run inference on trained model with the validation set\n",
    "testing(myModel, test_loader, y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe0e1c9-ebc3-4268-82c2-02e206b090ee",
   "metadata": {},
   "source": [
    "Below we calculate the Area Under Curve of the Receiver Operating Characterization Curve (AUR ROC Curve)\n",
    "\n",
    "This is a more robust measure of model quality than accuracy in a binary classification problem because it accounts for the likelihood that a \"positive\" guess by the model (e.g. there is a whale call in the audio sample) is a true positive. I.e. the model's positive guess is also labeled as a positive result by the class data.\n",
    "\n",
    "There are several functions for calculating ROC AUC scores by different libraries. Here we use the [sklearn.metrics roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) function. The closer the printed value is to 1, the better the model is. A value of 0.5 means the model is essentially randomly guessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d23db7f-1c41-48bf-8993-23d847236f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.8845\n"
     ]
    }
   ],
   "source": [
    "# Calculate the AUC score\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "print(\"AUC: {:.4f}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff382ab5-caf4-4e38-bcf5-257deb03fdce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
